{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How Multiprecision Leads to More Efficient and More Robust PDE Solvers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What is a multiprecision algorithm?\n",
    "\n",
    "- A multiprecision algorithm mixes the different floating point precisions.\n",
    "- This can be used for different purposes:\n",
    "  - It can lead to higher accuracy\n",
    "  - It can lead to more speed\n",
    "  \n",
    "Higher accuracy is obvious, speed is obvious when you decrease precision. But what I want to show today is that the opposite can be true: higher precision numbers can be used to increase performance!\n",
    "\n",
    "I will show how to use higher precision numbers to speed up calculations where you want results in `Float64`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ">There is a growing demand for and availability of multiprecision arithmetic: floating point arithmetic supporting multiple, possibly arbitrary, precisions. For an increasing body of applications, including in supernova simulations, electromagnetic scattering theory, and computational number theory, double precision arithmetic is insufficient to provide results of the required accuracy. On the other hand, for climate modelling and deep learning half precision (about four significant decimal digits) has been shown to be sufficient in some studies. We discuss a number of topics involving multiprecision arithmetic, including: The need for, availability of, and ways to exploit, higher precision arithmetic (e.g., quadruple precision arithmetic). How to derive linear algebra algorithms that will run in any precision, as opposed to be being optimized (as some key algorithms are) for double precision. For solving linear systems with the use of iterative refinement, the benefits of suitably combining three different precisions of arithmetic (say, half, single, and double). How a new form of preconditioned iterative refinement can be used to solve very ill conditioned sparse linear systems to high accuracy.\n",
    "\n",
    "Nick Higham, \"The Rise of Multiprecision Arithmetic\"\n",
    "\n",
    "http://ieeexplore.ieee.org/document/8023056/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why deal with floating point precision?\n",
    "\n",
    "Floating point errors are more prevelant than you may think."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999999999999999"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.1102230246251565e-16"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 - (0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1 + 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Catastrophic Cancellation\n",
    "\n",
    "- When floating point numbers are close to each other, the largest decimal places are cancelled, leaving you with less significant digits.\n",
    "- 64-bit floating point numbers have about 16 significant digits to start with.\n",
    "\n",
    "Some numerical algorithms are \"numerically unstable\" because they are written in a way that too many significant digits get lost.\n",
    "\n",
    "$$ \\gamma(z) = z^{-3} [-4-3z-z^2+exp(z)(4-z)] $$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14-element Array{Float64,1}:\n",
       "    0.154845  \n",
       "    0.16658   \n",
       "    0.166666  \n",
       "    0.166667  \n",
       "    0.166089  \n",
       "    0.0       \n",
       " -888.178     \n",
       "    0.0       \n",
       "    0.0       \n",
       "    0.0       \n",
       "    0.0       \n",
       "    0.0       \n",
       "    0.0       \n",
       "   -8.88178e23"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "γ(z) = z^(-3) * (-4 -3z - z^2 + exp(z)*(4-z))\n",
    "[γ(z) for z in 10.0.^(0:-1:-13)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14-element Array{BigFloat,1}:\n",
       " 1.548454853771357060808624140579874932717412810998787249009028831722298910606636e-01\n",
       " 1.66580495025736765660523311962006075734059476323003292166828819775070315847974e-01 \n",
       " 1.666658305495932401730424115348913751840819271355059720847049530210180144521954e-01\n",
       " 1.666666583305549602182401879407417261222251394538301831669786332575823153978047e-01\n",
       " 1.666666665833305554960307539544751432963063004381003501477127675851005003988853e-01\n",
       " 1.666666666658333305555496031646825259038635361376663292982592885815571781557752e-01\n",
       " 1.666666666666583333305555549603173611110973324498456788369994396928826516439519e-01\n",
       " 1.666666666666665833333305555554960317450396825259038799571225792615706816533831e-01\n",
       " 1.666666666666666658333333305555555496031745932539682004465582307851582870418442e-01\n",
       " 1.66666666666666666658333333330555555554960317460247755321726926688492858950762e-01 \n",
       " 1.66666666666666666666583333333330555555555496047730271150873838582095341205687e-01 \n",
       " 1.666666666666666666666658333333333305555555658391515682956565937782255386370758e-01\n",
       " 1.66666666666666666666666658333333333330517635099288390863392357685503173699033e-01 \n",
       " 1.666666666666666666666666665833333332835143183761055314139840813341704868024641e-01"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[γ(z) for z in big(10.0).^(0:-1:-13)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Numerical analysis is an entire science built around fixing these problems\n",
    "\n",
    "- How do we know when a problem is going to happen?\n",
    "- How do we efficient avoid numerical problems?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Basic Multiprecision Algorithm: Neural Nets and Optimization\n",
    "\n",
    "Optimize:\n",
    "\n",
    "$$ f(x), x \\in D$$\n",
    "\n",
    "- You don't accumulate error into `x`, so you might as well make it a `Float32` or a `Float16`.\n",
    "- If necessary, upconvert to `Float64` inside of `f` to avoid catastrophic cancellation\n",
    "- Autodifferentiation computes gradients at machine accuracy\n",
    "- The standard training methods like Gradient Descent and ADAM are safe (when used with automatic differentiation), so this algorihtm is \"low precision safe\".\n",
    "\n",
    "This is why \"gamer GPUs\" can be used!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Avoiding Issues in Rootfinding Algorithms\n",
    "\n",
    "Solving $g(x)=0$\n",
    "\n",
    "Native way: iterately find the values of `x`.\n",
    "\n",
    "Issue: sensitivity of the function `g` to input values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "2.68435456e8\n",
      "4.02653184e8\n"
     ]
    }
   ],
   "source": [
    "g(x) = 1e24*x - 1e24\n",
    "\n",
    "x = 1.0\n",
    "println(g(x))\n",
    "x_next = nextfloat(x)\n",
    "println(g(x_next))\n",
    "x_next2 = nextfloat(x_next)\n",
    "println(g(x_next2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Application: Stiff ODE Solvers\n",
    "\n",
    "DiffEq's way:\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/1814174/35989884-411f3fee-0cb7-11e8-8d93-83ea9a4c3c5d.PNG\">\n",
    "\n",
    "Sundials ARKODE's way:\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/1814174/35989888-4230242a-0cb7-11e8-8f82-cff416444179.PNG\">\n",
    "\n",
    "DiffEqBenchmarks.jl show how you can be orders of magnitude faster and more robust by doing this correctly.\n",
    "\n",
    "Note: Sundials CVODE and IDA do the DiffEq way!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Now that we are familiar with the subject, let's look at a specific example\n",
    "\n",
    "Fourth-Order Time-Stepping for Stiff PDEs\n",
    "\n",
    "Aly-Khan Kassam and Lloyd N. Trefethen\n",
    "\n",
    "Solve a discretization of a semilinear PDE:\n",
    "\n",
    "$$ u_t = Lu + N(u,t) $$\n",
    "\n",
    "via ETDRK4. Famous paper shows how to solve this fast!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/1814174/35991438-12ffb882-0cbc-11e8-9ee4-18ed61f12e70.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Idea: define a few constants:\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/1814174/35991506-3d6d8716-0cbc-11e8-824f-29becfbca983.PNG\">\n",
    "\n",
    "and now your loop just has a few matrix multiplications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One little problem: those equations are not numerically stable (γ is the equation from before!)\n",
    "\n",
    "Solution: solve it using contour integration. Pick a contour the encloses the eigenvalues of $L$ and do:\n",
    "\n",
    "<img src=\"https://user-images.githubusercontent.com/1814174/35991440-13220798-0cbc-11e8-81aa-0460601e5285.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/1814174/35991442-133c6f8e-0cbc-11e8-9199-b90d402baee3.PNG\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14-element Array{BigFloat,1}:\n",
       " 1.548454853771357060808624140579874932717412810998787249009028831722298910606636e-01\n",
       " 1.66580495025736765660523311962006075734059476323003292166828819775070315847974e-01 \n",
       " 1.666658305495932401730424115348913751840819271355059720847049530210180144521954e-01\n",
       " 1.666666583305549602182401879407417261222251394538301831669786332575823153978047e-01\n",
       " 1.666666665833305554960307539544751432963063004381003501477127675851005003988853e-01\n",
       " 1.666666666658333305555496031646825259038635361376663292982592885815571781557752e-01\n",
       " 1.666666666666583333305555549603173611110973324498456788369994396928826516439519e-01\n",
       " 1.666666666666665833333305555554960317450396825259038799571225792615706816533831e-01\n",
       " 1.666666666666666658333333305555555496031745932539682004465582307851582870418442e-01\n",
       " 1.66666666666666666658333333330555555554960317460247755321726926688492858950762e-01 \n",
       " 1.66666666666666666666583333333330555555555496047730271150873838582095341205687e-01 \n",
       " 1.666666666666666666666658333333333305555555658391515682956565937782255386370758e-01\n",
       " 1.66666666666666666666666658333333333330517635099288390863392357685503173699033e-01 \n",
       " 1.666666666666666666666666665833333332835143183761055314139840813341704868024641e-01"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "γ(z) = z^(-3) * (-4 -3z - z^2 + exp(z)*(4-z))\n",
    "[γ(z) for z in big(10.0).^(0:-1:-13)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"https://user-images.githubusercontent.com/1814174/35991683-cc219d94-0cbc-11e8-9538-6e8ca2df1609.PNG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But does it actually work?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cheb(N)\n",
    "    N==0 && return (0,1)\n",
    "    x = cos.(pi*(0:N)/N)\n",
    "    c = [2; ones(N-1,1); 2].*(-1).^(0:N)\n",
    "    X = repmat(x,1,N+1)\n",
    "    dX = X-X'\n",
    "    D  = (c*(1./c)')./(dX+(eye(N+1)))      # off-diagonal entries\n",
    "    D  = D - Diagonal(sum(D,2)[:])                 # diagonal entries\n",
    "    D,x\n",
    "end\n",
    "\n",
    "N = 20\n",
    "D,x = cheb(N)\n",
    "x = x[2:N]\n",
    "w = .53*x + .47*sin.(-1.5*pi*x) - x # use w = u-x to make BCs homogeneous\n",
    "u = [1;w+x;-1]\n",
    "h=1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.362286 seconds (24.48 k allocations: 5.292 MiB)\n"
     ]
    }
   ],
   "source": [
    "function contour_integral(h,M=32,contour_length=30)\n",
    "    r = 30*exp.(1im*pi*((1:M)-.5)/M) # points along complex circle\n",
    "    L = D^2; L = .01*L[2:N,2:N] # 2nd-order differentiation\n",
    "    A = h*L\n",
    "    E = expm(A); E2 = expm(A/2);\n",
    "    I = eye(N-1); Z = zeros(N-1,N-1)\n",
    "    f1 = Z; f2 = Z; f3 = Z; Q = Z\n",
    "\n",
    "    for j = 1:M\n",
    "        z = r[j];\n",
    "        zIA = inv(z*I-A);\n",
    "        Q = Q + h*zIA*(exp(z/2)-1);\n",
    "        f1 = f1 + h*zIA*(-4-z+exp(z)*(4-3*z+z^2))/z^2;\n",
    "        f2 = f2 + h*zIA*(2+z+exp(z)*(z-2))/z^2;\n",
    "        f3 = f3 + h*zIA*(-4-3*z-z^2+exp(z)*(4-z))/z^2;\n",
    "    end\n",
    "    E,E2,real(f1/M),real(f2/M),real(f3/M),real(Q/M)\n",
    "end\n",
    "\n",
    "@time E,E2,f1,f2,f3,Q = contour_integral(h,32,15); # same as the paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Julia Approach\n",
    "\n",
    "- Julia compiles codes specifically for the input types\n",
    "- It will generate fast code, even for high precision numbers\n",
    "- Let's write a multiprecision algorithm that upscales some parts to BigFloats, calculates, then downscales back to Float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.347007 seconds (367.85 k allocations: 18.631 MiB, 2.82% gc time)\n"
     ]
    }
   ],
   "source": [
    "function multiprecision(_h)\n",
    "    h = big(_h)\n",
    "    L = big.(D^2); L = .01*L[2:N,2:N] # 2nd-order differentiation\n",
    "    A = h*L\n",
    "    E = big.(expm(Float64.(A))); E2 = big.(expm(Float64.(A/2)));\n",
    "    A = big.(A); L = big.(L)\n",
    "    coeff = h^(-2) * L^(-3)\n",
    "    A2 = A^2\n",
    "    Q = Float64.((E2-I)/L)\n",
    "    a = Float64.(coeff * (-4I - A + E*(4I - 3A  + A2)))\n",
    "    b = Float64.(coeff * (2I + A + E*(-2I + A)))\n",
    "    c = Float64.(coeff * (-4I - 3A - A2 + E*(4I-A)))\n",
    "    Float64.(E),Float64.(E2),a,b,c,Q\n",
    "end\n",
    "\n",
    "@time Ẽ,Ẽ2,ã,b̃,c̃,Q̃2 = multiprecision(h);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E diff: 0.0\n",
      "E2 diff: 0.0\n",
      "f1 diff: 1016.1988572940766\n",
      "f2 diff: 15.630680277416415\n",
      "f3 diff: 15.149833068765068\n",
      "Q diff: 9.257143425563889e-13\n"
     ]
    }
   ],
   "source": [
    "# Their algorithm against ours\n",
    "\n",
    "println(\"E diff: \", Float64.(norm(E-Ẽ, Inf)))\n",
    "println(\"E2 diff: \",Float64.(norm(E2-Ẽ2,Inf)))\n",
    "println(\"f1 diff: \",Float64.(norm(f1-ã, Inf)))\n",
    "println(\"f2 diff: \",Float64.(norm(f2-b̃, Inf)))\n",
    "println(\"f3 diff: \",Float64.(norm(f3-c̃, Inf)))\n",
    "println(\"Q diff: \", Float64.(norm(Q-Q̃2, Inf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## We differ by 1000 in norm on one of the operators? (This is confirmed by checking directly against the MATLAB code).\n",
    "\n",
    "Who's wrong?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  3.617924 seconds (31.98 M allocations: 1.457 GiB, 20.95% gc time)\n"
     ]
    }
   ],
   "source": [
    "# Their code, but upconverted to BigFloats\n",
    "\n",
    "function big_contour_integral(_h,M=32)\n",
    "    h = big(_h)\n",
    "    r = 50*exp.(1im*big.(pi)*((1:M)-.5)/M) # points along complex circle\n",
    "    L = big.(D^2); L = .01*L[2:N,2:N] # 2nd-order differentiation\n",
    "    A = h*L\n",
    "    E = big.(expm(Float64.(A))); E2 = big.(expm(Float64.(A/2)));\n",
    "    I = big.(eye(N-1)); Z = zeros(BigFloat,N-1,N-1)\n",
    "    f1 = Z; f2 = Z; f3 = Z; Q = Z\n",
    "\n",
    "    for j = 1:M\n",
    "        z = r[j];\n",
    "        zIA = inv(z*I-A);\n",
    "        Q = Q + h*zIA*(exp(z/2)-1);\n",
    "        f1 = f1 + h*zIA*(-4-z+exp(z)*(4-3*z+z^2))/z^2;\n",
    "        f2 = f2 + h*zIA*(2+z+exp(z)*(z-2))/z^2;\n",
    "        f3 = f3 + h*zIA*(-4-3*z-z^2+exp(z)*(4-z))/z^2;\n",
    "    end\n",
    "    E,E2,real(f1/M),real(f2/M),real(f3/M),real(Q/M)\n",
    "end\n",
    "\n",
    "@time E,E2,f1,f2,f3,Q = big_contour_integral(h,128);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E diff: 0.0\n",
      "E2 diff: 0.0\n",
      "f1 diff: 3.743791623349559e-9\n",
      "f2 diff: 1.3116559357892626e-9\n",
      "f3 diff: 2.199026197507343e-9\n",
      "Q diff: 1.699216236371553e-14\n"
     ]
    }
   ],
   "source": [
    "println(\"E diff: \", Float64.(norm(E-Ẽ, Inf)))\n",
    "println(\"E2 diff: \",Float64.(norm(E2-Ẽ2,Inf)))\n",
    "println(\"f1 diff: \",Float64.(norm(f1-ã, Inf)))\n",
    "println(\"f2 diff: \",Float64.(norm(f2-b̃, Inf)))\n",
    "println(\"f3 diff: \",Float64.(norm(f3-c̃, Inf)))\n",
    "println(\"Q diff: \", Float64.(norm(Q-Q̃2, Inf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## After we change their algorithm (essentially copy/paste from the paper) to use BigFloat, we match!\n",
    "\n",
    "How sensitive was their original algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.300812 seconds (1.41 k allocations: 4.362 MiB, 64.55% gc time)\n",
      "f1 diff: 1016.1988572940766\n",
      "  0.498916 seconds (5.25 k allocations: 16.598 MiB)\n",
      "f1 diff: 7.312173475863082e-7\n"
     ]
    }
   ],
   "source": [
    "# Up the integration points to 128. They argue you can use a small number because it converges exponentially,\n",
    "# this is proven false by the actual calculation\n",
    "\n",
    "# Radius 15\n",
    "@time E,E2,f1,f2,f3,Q = contour_integral(h,32,15); \n",
    "println(\"f1 diff: \",Float64.(norm(f1-ã, Inf)))\n",
    "\n",
    "@time E,E2,f1,f2,f3,Q = contour_integral(h,128,15); \n",
    "println(\"f1 diff: \",Float64.(norm(f1-ã, Inf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  294.086 ms (5236 allocations: 16.60 MiB)\n",
      "  143.624 ms (360727 allocations: 18.25 MiB)\n"
     ]
    }
   ],
   "source": [
    "# Timings\n",
    "using BenchmarkTools\n",
    "@btime E,E2,f1,f2,f3,Q = contour_integral(h,128,15);\n",
    "@btime Ẽ,Ẽ2,ã,b̃,c̃,Q̃2 = multiprecision(h);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Result\n",
    "\n",
    "- Their algorithm takes 128 matrix inversions\n",
    "  - This exact number is problem-dependent and hard to find!\n",
    "- Ours does 1\n",
    "- Ours can easily be further optimized to be only 4 allocations total\n",
    "- Even without optimizations, we're already faster on a small problem\n",
    "  - Their algorithm scales worse due to the inversions!\n",
    "  \n",
    "Thus this multiprecision algorithm, taking a tour through BigFloats, is both faster and more robust!\n",
    "\n",
    "We could also use things like DoubleFloats.jl to speed this up even more.\n",
    "\n",
    "## Note\n",
    "\n",
    "This will not make all problems faster. It can be dependent on the size of the matrix, the optimality of the higher precision number implementation, the condition number of the matrix, etc. Some cases the contour integral approach will be better, while in others the BigFloat method will be. But for sure the BigFloat method can be easier to get right the first time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "- Higher and lower precision numbers can allow you to optimize codes even when all you care about is 64-bit floats\n",
    "- Multiprecision algorithms can be much simpler, more robust to numerical issues, and faster\n",
    "- Julia's generic programming tools are well-designed for fast multiprecision computing\n",
    "\n",
    "How much of numerical analysis has been sidetracked because we didn't have Julia? This is an entire subject to research which is not suited for R/MATLAB/Python, and is only possible in C++ if you're a programming genius. However, it's widely accessible in Julia."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Julia 0.6.2",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
